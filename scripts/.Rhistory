}
set.seed(42)
### Function call paramters: function(model_type, feature_set_name, selected_features, tuneGrid)
# ###########################
# #### Regression
# ###########################
#
# # Train Logistic Regression With & Without SDOH
# train_and_assign("glm", "Logistic Regression SDOH", features_sdoh, NULL)
# train_and_assign("glm", "Logistic Regression No SDOH", features_no_sdoh, NULL)
#
# # Verify that all models have been created
# print(ls(pattern = "_model_"))
#
# tuneGrid_glmnet <- expand.grid(
#   alpha = c(0, 0.5, 1),  # 0 = Ridge, 1 = Lasso, 0.5 = Elastic Net
#   lambda = seq(0.001, 0.1, length = 10)
# )
#
# # Train Ridge, Lasso, and Elastic Net
# train_and_assign("glmnet", "Ridge Regression", features_sdoh, tuneGrid_glmnet)
# train_and_assign("glmnet", "Lasso Regression", features_sdoh, tuneGrid_glmnet)
# train_and_assign("glmnet", "Elastic Net", features_sdoh, tuneGrid_glmnet)
#
###########################
#### Bayesian Logistic Regression
###########################
# Train Bayesian GLM With & Without SDOH
train_and_assign("bayesglm", "Bayesian Logistic Regression SDOH", features_sdoh, NULL)
knitr::opts_chunk$set(echo = TRUE)
# install.packages(c("arm","gbm","class","elmNN","kernlab")
# data mgmt & analysis
library(here)
library(bigrquery)
library(DBI)
library(tidyverse)
library(GGally)
library(lattice)
library(MASS)
library(dplyr)
library(car)
library(corrplot)
library(gridExtra)
# Training, validation and tuning
library(caret)
library(yardstick)
library(pROC)
library(PRROC)
library(smotefamily)
library(SHAPforxgboost)
# Model
library(xgboost)      # Extreme Gradient Boosting
library(gbm)          # Gradient Boosting Machines Log Reg
library(randomForest) # Random Forest Tree
library(glmnet)       # Logistic Regression
library(arm)          # Bayesian Logistic Regression
library(nnet)         # Neural Network
library(class)        # K-Nearest Neighbors
library(kernlab)      # SVM's
library(e1071)        # SVM's
library(naivebayes)   # Naive Bayes
# visualization
library(ggplot2)
library(ggcorrplot)
library(gridExtra)
# library(HH)
# library(ISLR2)
# library(faraway)
# Load utilities
source(here("scripts", "data_utils.R"))
# Connect to BigQuery
source(here("scripts", "bigquery_connect.R"))
# Run SQL and process data
source(here("scripts", "process_data.R"))
# rm(list = setdiff(ls(), lsf.str()))  # Removes all objects but keeps functions
# gc()  # Run garbage collection
library(caret)
library(xgboost)
library(randomForest)
library(e1071) # SVM
library(nnet)
customSummary <- function(data, lev = NULL, model = NULL) {
obs <- data$obs
prob_yes <- data$Yes
pred_class <- ifelse(prob_yes >= 0.5, "Yes", "No")
pred_class <- factor(pred_class, levels = lev)
# Compute Metrics
roc_auc <- yardstick::roc_auc_vec(truth = obs, estimate = prob_yes, event_level = "second")
pr_auc <- yardstick::pr_auc_vec(truth = obs, estimate = prob_yes, event_level = "second")
precision <- yardstick::precision_vec(truth = obs, estimate = pred_class, event_level = "second")
recall <- as.numeric(yardstick::recall_vec(truth = obs, estimate = pred_class, event_level = "second"))
f1_score <- yardstick::f_meas_vec(truth = obs, estimate = pred_class, event_level = "second")
sens <- yardstick::sens_vec(truth = obs, estimate = pred_class, event_level = "second")
spec <- yardstick::spec_vec(truth = obs, estimate = pred_class, event_level = "second")
accuracy <- yardstick::accuracy_vec(truth = obs, estimate = pred_class)
kappa <- yardstick::kap_vec(truth = obs, estimate = pred_class)  # Add Cohen's Kappa
# Return all metrics
c(Accuracy = accuracy, Kappa = kappa, ROC = roc_auc, PR_AUC = pr_auc,
Precision = precision, Recall = recall, F1 = f1_score, Specificity = spec, Sensitivity = sens)
}
# Generalized Training Function with Time-Based Splitting & Correct Summary Function
train_single_model <- function(model_name, target, feature_set_name, selected_features, dataset, dataset_name, csv_path, tuneGrid = NULL, time_slices = 5) {
# Sort dataset by time (to ensure chronological training)
dataset <- dataset[order(dataset$admittime), ]
# Define Time-Based Cross-Validation with Correct Summary Function
ctrl <- if (is.numeric(dataset[[target]])) {
trainControl(
method = "timeslice",
initialWindow = round(0.7 * nrow(dataset)),  # Train on 70% of historical data
#horizon = round(0.3 * nrow(dataset)),        # Test on 30% of future data
horizon = round(0.3 * nrow(dataset)),  # Increase test set to 30% of data,
fixedWindow = TRUE,
#skip = round(nrow(dataset) / (time_slices + 1)),  # Number of skipped observations per window
skip = max(1, round(nrow(dataset) / (time_slices + 1))),
summaryFunction = defaultSummary,
savePredictions = "final",
verboseIter = TRUE
)
} else {
trainControl(
method = "timeslice",
initialWindow = round(0.7 * nrow(dataset)),
horizon = round(0.3 * nrow(dataset)),
#horizon = min(100, round(0.2 * nrow(dataset))),
fixedWindow = TRUE,
#skip = round(nrow(dataset) / (time_slices + 1)),  # Number of skipped observations per window
skip = max(1, round(nrow(dataset) / (time_slices + 1))),
classProbs = TRUE,
summaryFunction = customSummary,
savePredictions = "final",
sampling = "smote",
verboseIter = TRUE
)
}
# Construct formula
formula <- as.formula(paste(target, "~", paste(selected_features, collapse = " + ")))
# Train Model
model <- train(formula, data = dataset, method = model_name, trControl = ctrl, tuneGrid = tuneGrid)
results <- model$results
# Timestamp of Model Run
model_timestamp <- format(Sys.time(), "%Y-%m-%d %H:%M:%S")  # Format as "YYYY-MM-DD HH:MM:SS"
# Extract AIC & BIC if applicable
final_model <- model$finalModel
results$AIC <- if ("glm" %in% class(final_model) || "lm" %in% class(final_model)) AIC(final_model) else NA
results$BIC <- if ("glm" %in% class(final_model) || "lm" %in% class(final_model)) BIC(final_model) else NA
all_metrics <- c(
"Accuracy",    # Standard classification metric (useful for balanced datasets)
"Kappa",       # Agreement metric useful for imbalanced datasets
"ROC",         # Area under the ROC curve (classification)
"PR_AUC",      # Precision-Recall AUC (better for highly imbalanced datasets)
"Sensitivity", # Sensitivity (Recall) - True Positive Rate
"Specificity", # Specificity - True Negative Rate
"Precision",   # Precision (Positive Predictive Value)
"Recall",      # Recall (Sensitivity)
"F1",          # F1-Score (harmonic mean of Precision & Recall)
"RMSE",        # Root Mean Squared Error (regression)
"Rsquared",    # RÂ² - Variance explained (regression)
"MAE",         # Mean Absolute Error (regression)
"AIC",         # Akaike Information Criterion
"BIC"          # Bayesian Information Criterion
)
# Ensure all metrics exist, setting missing ones to NA
for (metric in all_metrics) {
if (!(metric %in% colnames(results))) {
results[[metric]] <- NA  # Fill missing columns with NA for consistency
}
}
# Ensure numerical values are stored with higher precision
results[all_metrics] <- lapply(results[all_metrics], function(x) {
round(as.numeric(x), 6)  # Increase to 6 decimal places
})
# Store tuning parameters for each row in results
if (!is.null(tuneGrid)) {
results$Tuning_Params <- apply(model$results[, names(tuneGrid), drop = FALSE], 1, function(row) {
paste(names(row), "=", row, collapse = "; ")
})
} else {
results$Tuning_Params <- "Default"
}
# Standardize Output Format
results$Data_Set <- dataset_name
results$Timestamp <- model_timestamp
results$Target <- target
results$Model <- model_name
results$Feature_Set <- feature_set_name
results$Features_Used <- paste(selected_features, collapse = ", ")
results$Validation_Type <- "Time-Based CV"
results$Time_Slices <- time_slices
results$Description <- paste("Model trained on", feature_set_name, "for", target)
final_results <- results[, c("Timestamp","Data_Set", "Target", "Model", "Feature_Set", "Features_Used", "Validation_Type", "Time_Slices", "Tuning_Params", "Description", all_metrics), drop = FALSE]
# Save to CSV (Append or Create New)
if (file.exists(csv_path)) {
existing_results <- read.csv(csv_path)  # Read existing results
final_results <- rbind(existing_results, final_results)  # Append new results
}
write.csv(final_results, csv_path, row.names = FALSE)  # Write everything back
cat("\n Model results saved to:", csv_path, "\n")
print(final_results)
return(model)
}
#### Model Setup
train_and_assign <- function(model_type, feature_set_name, selected_features, tuneGrid) {
# Define dynamic model name
model_name <- paste0(model_type, "_", gsub(" ", "_", tolower(feature_set_name)), "_", target, "_", model_run)
# Train model and assign it dynamically
assign(model_name, train_single_model(
model_name = model_type,
target = target,
feature_set_name = feature_set_name,
selected_features = selected_features,
dataset = data,
dataset_name = dataset_name,
csv_path <- here("results", file_name),
# csv_path = file_name,
time_slices = slices,
tuneGrid = tuneGrid
), envir = .GlobalEnv)  # Assign the model to the global environment
# Return the model name for debugging
return(model_name)
}
set.seed(42)
data <- data_hosp
dataset_name <- "Samp_50_hosp"
target <- "length_of_stay"  ### mortality_30_day, readmitted , hospital_expire_flag, length_of_stay
file_name <- "HOSP3.csv"
slices <- 4
model_run <- "hosp4"
set.seed(42)
### Function call paramters: function(model_type, feature_set_name, selected_features, tuneGrid)
# ###########################
# #### Regression
# ###########################
#
# # Train Logistic Regression With & Without SDOH
# train_and_assign("glm", "Logistic Regression SDOH", features_sdoh, NULL)
# train_and_assign("glm", "Logistic Regression No SDOH", features_no_sdoh, NULL)
#
# # Verify that all models have been created
# print(ls(pattern = "_model_"))
#
# tuneGrid_glmnet <- expand.grid(
#   alpha = c(0, 0.5, 1),  # 0 = Ridge, 1 = Lasso, 0.5 = Elastic Net
#   lambda = seq(0.001, 0.1, length = 10)
# )
#
# # Train Ridge, Lasso, and Elastic Net
# train_and_assign("glmnet", "Ridge Regression", features_sdoh, tuneGrid_glmnet)
# train_and_assign("glmnet", "Lasso Regression", features_sdoh, tuneGrid_glmnet)
# train_and_assign("glmnet", "Elastic Net", features_sdoh, tuneGrid_glmnet)
#
###########################
#### Bayesian Logistic Regression
###########################
# Train Bayesian GLM With & Without SDOH
train_and_assign("bayesglm", "Bayesian Logistic Regression SDOH", features_sdoh, NULL)
train_and_assign("bayesglm", "Bayesian Logistic Regression No SDOH", features_no_sdoh, NULL)
###########################
#### Gradient Boosting Decision Trees
###########################
# tuneGrid = expand.grid(
#   nrounds = c(50, 100, 200),
#   max_depth = c(3, 6, 9),
#   eta = c(0.01, 0.1, 0.3),
#   gamma = c(0, 1, 5),
#   colsample_bytree = c(0.8, 1),
#   min_child_weight = c(1, 5),
#   subsample = c(0.8, 1)
# )
tuneGrid_xgb <- expand.grid(
nrounds = c(200),
max_depth = c(6),
eta = c(0.3),
gamma = c(0),
colsample_bytree = c(1),
min_child_weight = c(1),
subsample = c(1)
)
# Train XGBoost With & Without SDOH
train_and_assign("xgbTree", "Full Features SDOH", features_sdoh, tuneGrid_xgb)
train_and_assign("xgbTree", "Full Features No SDOH", features_no_sdoh, tuneGrid_xgb)
###########################
#### Gradient Boosting Machines
###########################
# tuneGrid_gbm <- expand.grid(
#   interaction.depth = c(1, 3, 5),
#   n.trees = c(50, 100, 200),
#   shrinkage = c(0.1, 0.01),
#   n.minobsinnode = 10
# )
tuneGrid_gbm <- expand.grid(
interaction.depth = c(3),
n.trees = c(100),
shrinkage = c(0.1),
n.minobsinnode = 10
)
# Train GBM With & Without SDOH
train_and_assign("gbm", "Gradient Boosting SDOH", features_sdoh, tuneGrid_gbm)
train_and_assign("gbm", "Gradient Boosting No SDOH", features_no_sdoh, tuneGrid_gbm)
# ###########################
# #### Random Forest
# ###########################
#
# #tuneGrid_rf <- expand.grid(mtry = c(2, 5, 10))
# tuneGrid_rf <- expand.grid(mtry = 2)
#
# # Train Random Forest With & Without SDOH
# train_and_assign("rf", "Random Forest SDOH", features_sdoh, tuneGrid_rf)
# train_and_assign("rf", "Random Forest No SDOH", features_no_sdoh, tuneGrid_rf)
#
library(ggplot2)
evaluate_model <- function(model, dataset, target) {
# Generate Predictions
predictions <- predict(model, newdata = dataset)
actuals <- dataset[[target]]
model_name <- deparse(substitute(model))
# Create a DataFrame for Plotting
results_df <- data.frame(
Predicted = predictions,
Actual = actuals,
Residuals = actuals - predictions
)
# Plot: Actual vs. Predicted
p1 <- ggplot(results_df, aes(x = Predicted, y = Actual)) +
geom_point(alpha = 0.3) +
geom_smooth(method = "lm", col = "red") +
ggtitle(paste("Actual vs. Predicted LOS - ", model_name)) +
xlab("Predicted LOS") +
ylab("Actual LOS")
# Plot: Residual Distribution
p2 <- ggplot(results_df, aes(x = Residuals)) +
geom_histogram(binwidth = 1, fill = "blue", alpha = 0.5) +
ggtitle(paste("Residual Distribution - ", model_name)) +
xlab("Residual (Actual - Predicted)") +
ylab("Frequency")
# Display Plots
print(p1)
print(p2)
}
model_list <- hosp2
# all models
all_models <- mget(ls(pattern = "_"), envir = .GlobalEnv)
# print(names(model_list))
# # Model types models("xgbTree" "rf" "nnet" "glm")
xgb_models <- mget(ls(pattern = "^xgbTree_"), envir = .GlobalEnv)
# rf_models <- mget(ls(pattern = "^rf_"), envir = .GlobalEnv)
nnet_models <- mget(ls(pattern = "^nnet_"), envir = .GlobalEnv)
glm_models <- mget(ls(pattern = "^glm_"), envir = .GlobalEnv)
# bayes_models <- mget(ls(pattern = "^bayesglm_"), envir = .GlobalEnv)
gbm_models <- mget(ls(pattern = "^gbm_"), envir = .GlobalEnv)
#
# # models by target
# mort_models <- mget(ls(pattern = "mortality_30_day"), envir = .GlobalEnv)
# readmit_models <- mget(ls(pattern = "readmitted"), envir = .GlobalEnv)
# expire_models <- mget(ls(pattern = "hospital_expire_flag"), envir = .GlobalEnv)
# los_models <- mget(ls(pattern = "length_of_stay"), envir = .GlobalEnv)
hosp3 <- mget(ls(pattern = "hosp3"), envir = .GlobalEnv)
hosp2 <- mget(ls(pattern = "hosp2"), envir = .GlobalEnv)
# search_string <- "features"
#
# # Filter model names that contain the search string
# matching_models <- all_models[grepl(search_string, names(all_models), ignore.case = TRUE)]
model_list <- hosp2
for (model_name in names(model_list)) {
cat("\nEvaluating Model:", model_name, "\n")
evaluate_model(model_list[[model_name]], data_hosp, "length_of_stay")
}
knitr::opts_chunk$set(echo = TRUE)
# install.packages(c("arm","gbm","class","elmNN","kernlab")
# data mgmt & analysis
library(here)
library(bigrquery)
library(DBI)
library(tidyverse)
library(GGally)
library(lattice)
library(MASS)
library(dplyr)
library(car)
library(corrplot)
library(gridExtra)
# Training, validation and tuning
library(caret)
library(yardstick)
library(pROC)
library(PRROC)
library(smotefamily)
library(SHAPforxgboost)
# Model
library(xgboost)      # Extreme Gradient Boosting
library(gbm)          # Gradient Boosting Machines Log Reg
library(randomForest) # Random Forest Tree
library(glmnet)       # Logistic Regression
library(arm)          # Bayesian Logistic Regression
library(nnet)         # Neural Network
library(class)        # K-Nearest Neighbors
library(kernlab)      # SVM's
library(e1071)        # SVM's
library(naivebayes)   # Naive Bayes
# visualization
library(ggplot2)
library(ggcorrplot)
library(gridExtra)
# library(HH)
# library(ISLR2)
# library(faraway)
# Load utilities
source(here("scripts", "data_utils.R"))
# Connect to BigQuery
source(here("scripts", "bigquery_connect.R"))
# Run SQL and process data
source(here("scripts", "process_data.R"))
# rm(list = setdiff(ls(), lsf.str()))  # Removes all objects but keeps functions
# gc()  # Run garbage collection
source(here("scripts", "figs.funcs.R"))
# Display numeric target histograms
grid.arrange(grobs = plots$hist_list, ncol = 1)
# Display binary target bar charts
grid.arrange(grobs = plots$bar_list, ncol = 3)
# Print summary table
knitr::kable(plots$summary_table, format = "markdown")
print(figure.02)
print(patient_history)
### "mortality_30_day", "readmitted", "hospital_expire_flag")
set.seed(42)
train_function = "train_single_model2"
data <- data_hosp
dataset_name <- "Samp_50_hosp"
target <- "mortality_30_day"
file_name <- "HOSP5.csv"
slices <- 4
model_run <- "modelfunc2"
split_type <- "patient"
train_size <- 0.7
cv_splits <- 2
final_model <- FALSE
train_and_assign(
model_type = "glm",
feature_set_name = "HOSP_ICU_SDOH",
selected_features = features_hosp_icu_sdoh,
tuneGrid = NULL
)
train_and_assign(
model_type = "glm",
feature_set_name = "NO_SDOH",
selected_features = features_no_sdoh,
tuneGrid = NULL
)
train_and_assign(
model_type = "gbm",
feature_set_name = "HOSP_ICU_SDOH",
selected_features = features_hosp_icu_sdoh,
tuneGrid = NULL
)
train_and_assign(
model_type = "gbm",
feature_set_name = "NO_SDOH",
selected_features = features_no_sdoh,
tuneGrid = NULL
)
# all models
all_models <- mget(ls(pattern = "_"), envir = .GlobalEnv)
# print(names(model_list))
# # Model types models("xgbTree" "rf" "nnet" "glm")
xgb_models <- mget(ls(pattern = "^xgbTree_"), envir = .GlobalEnv)
rf_models <- mget(ls(pattern = "^rf_"), envir = .GlobalEnv)
nnet_models <- mget(ls(pattern = "^nnet_"), envir = .GlobalEnv)
glm_models <- mget(ls(pattern = "^glm_"), envir = .GlobalEnv)
bayes_models <- mget(ls(pattern = "^bayesglm_"), envir = .GlobalEnv)
gbm_models <- mget(ls(pattern = "^gbm_"), envir = .GlobalEnv)
# models by target
mort_models <- mget(ls(pattern = "mortality_30_day"), envir = .GlobalEnv)
readmit_models <- mget(ls(pattern = "readmitted"), envir = .GlobalEnv)
expire_models <- mget(ls(pattern = "hospital_expire_flag"), envir = .GlobalEnv)
los_models <- mget(ls(pattern = "length_of_stay"), envir = .GlobalEnv)
hosp3 <- mget(ls(pattern = "hosp3"), envir = .GlobalEnv)
ICU2 <- mget(ls(pattern = "ICU2"), envir = .GlobalEnv)
search_string <- "modelfunc2"
# Filter model names that contain the search string
matching_models <- all_models[grepl(search_string, names(all_models), ignore.case = TRUE)]
source(here("scripts", "model_utils.R"))
source(here("scripts", "model_utils.R"))
save_models(matching_models, subfolder = "Mort30_v1")
# save_models(models)  # Saves inside "models/"
plot_roc_curves(
models = matching_models,
model_names = sapply(matching_models, function(model) model$method),
target = "hospital_expire_flag",
plot_title = "Hospital Mortality Prediction"
)
# plot_roc_curves(
#   models = readmit_models,
#   model_names = sapply(readmit_models, function(model) model$method),
#   target = "readmitted",
#   plot_title = "Hospital Readmission Prediction"
# )
# Hospital Mortality Models
plot_pr_curves(
models = matching_models,
model_names = sapply(matching_models, function(model) model$method),
target = "hospital_expire_flag",
plot_title = "Hospital Mortality Prediction"
)
# # Readmitted Models
# plot_pr_curves(
#   models = readmit_models,
#   model_names = sapply(readmit_models, function(model) model$method),
#   target = "readmitted",
#   plot_title = "Hospital Readmission Prediction"
# )
source(here("scripts", "model_utils.R"))
save_models(matching_models, subfolder = "Mort30_v2")
# save_models(models)  # Saves inside "models/"
